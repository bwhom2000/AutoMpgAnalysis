---
title: "Sta 141A Project: Auto-MPG Analysis"
date: "11/2/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes: \usepackage[labelformat=empty]{caption}


---
| Name                        | Email                                              | Contributions                                                                                                  |
|-----------------------------|----------------------------------------------------|----------------------------------------------------------------------------------------------------------------|
| Aditi Goyal                 | adigoyal@ucdavis.edu                               | Report Writing, table formatting. \vspace{10pt}                                |
| Brandon Hom                 | bwhom@ucdavis.edu                                  | Unsupervised and supervised learning analyses,  web app, data visualization, tables \vspace{10pt}                            |
|  Tammie Tam                 | tastam@ucdavis.edu                                 |Report Writing , plot formatting  |
```{=tex}
\tableofcontents
\newpage
```

```{r setup, include=FALSE}
#global options
# keeps this here to remove comments from knitted output. 
knitr::opts_chunk$set(comments=NA)
knitr::opts_chunk$set(echo=F)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```
```{r,include=F}
library(tidyverse)
library(knitr)
library(leaps)
library(GGally)
library(ggbiplot)
library(caret)
library(RColorBrewer)
library(dendextend)
library(cowplot)
library(kableExtra)
```

# Introduction 
Fuel economy is defined as how much a car can travel per volume of fuel, which is usually measured as miles per gallon (mpg). Cars with low mpg have good fuel economy and vice versa. Logically, consumers prefer to own cars with good fuel economy, since they would spend less money on gas. In addition, cars with poor fuel economy consumes more gas, which, in turn, contributes to global warming; gas is ultimately a limited resource [1]. The effects of pollution produced from the consumption of gas can be mitigated by using cars with good fuel economy.
The ability to predict a car's fuel economy based on a set of a given car's characteristics, or information on car models with good fuel economy, can allow individual to make a more informed decision when purchasing a car - a decision that can have a positive impact on both spending and global warming. 

# Dataset Description
The data set on fuel economy was obtained from kaggle: https://www.kaggle.com/uciml/autompg-dataset. Although it is data from the late 1900s, the features within the data set can provide us insight on what variables have impact on fuel economy.The dimensions of the data set are 398 by 9, meaning that we have 9 features within our data set. `Model year` is not useful for our purposes, so it will be dropped during the analysis. `Weight`, `acceleration`, `horsepower`, `displacement`, `cylinders` are all numerical features, while `origin` is categorical. `Weight`, `acceleration`, and `horsepower` are self-explanatory. `Cylinders` are indicative of the power of an engine, where more cylinders means more power but more consumption of gas. `Displacement` refers to how much volume of air and fuel moved through the cylinders of the engine.

# Research Questions 


* Is it appropriate to fit a linear model to this data? If so, what numerical variables have the most impact on mpg? For example, if increasing weight contributes the most to MPG, individuals should be wary about purchasing heavy cars, since it will lead to more consumption of fuel.

* Is it possible to build a predictive model with reasonable performance to predict a car's fuel economy? If it is possible, individuals will be able to make better car-purchasing decisions by inputting a car's features into the model and getting an estimated mpg. 
 
* The origin column has Europe, Japan and USA encoded. Are cars from these regions similar, or are they completely different? Is there a region that tends to make cars with good fuel economy? What about a region that produces cars with poor fuel economy?  

* What brand of cars are similar in terms of fuel economy and other features such as weight? Knowing this information can allow individuals to potentially buy cars with desired feature levels or even avoid buying cars with poor fuel economy.  

These questions may be answered by using unsupervised and supervised learning methods. 

\newpage
# \textcolor{violet}{Unsupervised Learning Analysis}

## Hierarchical Clustering Analysis
```{r,warning=F}
# data cleaning up
data <- read.csv('auto-mpg.csv')
#convert horsepower chr->dbl 
data$horsepower <- as.numeric(data$horsepower)
#remove rows with missing values
data <- na.omit(data)
#translate origin numbers to country strings
data$origin <- ifelse(data$origin==1,"USA",ifelse(data$origin==2,"Europe","Japan"))
data$origin <- as.factor(data$origin)
#cylinders count for 3 and 5 low combine with 4 and 6 respectively
data$cylinders <- replace(data$cylinders,data$cylinders %in% c(3,5),c(4,6))
data$cylinders <- as.factor(data$cylinders)
#remove model.year, not interested in this feature 
data <- data[-c(7)]
data$car.name <- word(data$car.name,1)
#car.name fix typos
data$car.name[160] <- "chevrolet"
data$car.name[330] <- "volkswagen"
data$car.name[82] <- "toyota"
```



```{r,out.height="50%",out.width="80%",fig.cap="\\textcolor{blue}{Figure 1: Hierarchical Clustering results}"}
h.clustering.complete <- hclust(dist(scale(data[-c(2,7,8)])),method="complete") %>% as.dendrogram() %>% color_branches(k=3)
color.order <- as.numeric(data$origin)
colors.dendro <- color.order[order.dendrogram(h.clustering.complete)]
colors.dendro <- ifelse(colors.dendro==3,"green",ifelse(colors.dendro==2,"red","blue"))
labels_colors(h.clustering.complete) <- colors.dendro
h.clustering.complete <- h.clustering.complete %>% set("labels_col",colors.dendro)
plot(h.clustering.complete,
     main="Dendrogram of clustered cars colored by country",
     ylab="Height",
     xlab="Country")
legend(x=290,y=9,legend=c("USA","Europe","Japan"),fill=c("green","red","blue"))
```


The numerical features were scaled to have standard deviation one and complete linkage was used to cluster the cars. From the dendrogram above, there seems to be three clusters. The cluster colored with red branches consists entirely of cars from the United States, while the other two clusters are mixed.

To gain better insight on the clusters, the mean of the numerical features was calculated for the clusters and within clusters. 
```{r}
hclust.data <- data.frame(data,cluster=cutree(h.clustering.complete,h=5)) 
hclust.data.clusters <- data.frame(data,cluster=cutree(h.clustering.complete,h=5)) %>% count(c('cluster'))
hclust.in.clusters <- data.frame(data,cluster=cutree(h.clustering.complete,h=5)) %>% count(c('cluster','origin'))

#cluster analysis 
c.1 <- hclust.data %>% filter(cluster==1 ) %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )

c.2 <- hclust.data %>% filter(cluster==2 ) %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )

c.3 <- hclust.data %>% filter(cluster==3 ) %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )

clusters.data <- rbind(c.1,c.2,c.3)
hclust.data.clusters <- cbind(hclust.data.clusters,clusters.data)

# Within cluster analysis 
US.1 <-  hclust.data %>% filter(cluster==1 & origin=="USA") %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )
EU.2 <-  hclust.data %>% filter(cluster==2 & origin=="Europe") %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )
JN.2 <-  hclust.data %>% filter(cluster==2 & origin=="Japan") %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )
US.2 <-  hclust.data %>% filter(cluster==2 & origin=="USA") %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )
EU.3 <-  hclust.data %>% filter(cluster==3 & origin=="Europe") %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )
JN.3 <-  hclust.data %>% filter(cluster==3 & origin=="Japan") %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )
US.3 <-  hclust.data %>% filter(cluster==3 & origin=="USA") %>% summarise(mean.mpg=mean(mpg),
                                                                   mean.displacement=mean(displacement),
                                                                   mean.horsepower=mean(horsepower),
                                                                   mean.weight=mean(weight),
                                                                   mean.acceleration=mean(acceleration)
                                                                    )
in.clusters.data <- rbind(US.1,EU.2,JN.2,US.2,EU.3,JN.3,US.3)
hclust.in.clusters <- cbind(hclust.in.clusters,in.clusters.data)
kable(hclust.data.clusters,format="latex",booktabs=T,longtable=T,caption = "Table 1: Cluster Analysis") %>% kable_styling(font_size = 7,latex_options = "HOLD_position")
#hclust.data.clusters
#hclust.in.clusters
```
```{r}
kable(hclust.in.clusters,format="latex",booktabs=T,longtable=T,caption="Table 2: Within Cluster Analysis") %>% kable_styling(font_size = 7,latex_options = "HOLD_position")
```




## Principal Component analysis 

```{r}
pcs.out <- prcomp(data[-c(2,7,8)],scale.=T)
pcs.dat <- data.frame(rownames(pcs.out$rotation),pcs.out$rotation)
colnames(pcs.dat)[1] <- "Features"
pcs.importance <- data.frame(summary(pcs.out)[6])
pcs.importance <- cbind(c("Standard deviation","Proportion of Variance","Cumulative Proportion"),pcs.importance)
colnames(pcs.importance) <- c("Metrics","PC1","PC2","PC3","PC4","PC5")
```
```{r} 
cols <- brewer.pal(3, "Dark2")
```

```{r}
PCA.interp <- data.frame(Position=c("Top Right","Top Left","Bottom Right","Bottom Left"),
           MPG=c("Larger","Lower","Larger","Lower"),
           Displacement=c("Lower","Larger","Lower","Larger"),
           Horsepower=c("Lower","Larger","Lower","Larger"),
           Weight=c("Lower","Larger","Lower","Larger"),
           Acceleration=c("Lower","Larger","Lower","Larger"))
```


```{r}
PCA.interp
```


\newpage
```{r,out.width="50%",out.height="25%",fig.show='hold'}

ggbiplot(pcs.out,labels = data$car.name,groups=data$origin,obs.scale = 1,labels.size = 2.3)+
  geom_hline(yintercept = 0,col="hotpink3")+
  geom_vline(xintercept = 0,col="hotpink3")+
  ylim(0,1.8)+
  xlim(-5,0)+
  theme_light()+
  theme(plot.title=element_text(hjust=.5,size=20),
        axis.text = element_text(size=15)
        )+
  labs(title="Top left",
       )+
  scale_color_manual(values=cols)+ theme(legend.box.background = element_rect(linetype="solid", colour ="hotpink3", size=1.25),
        legend.title = element_text(face="bold", hjust = .5),
        legend.text = element_text(face="bold"))+
  guides(colour=guide_legend("Country"))



ggbiplot(pcs.out,labels = data$car.name,groups=data$origin,obs.scale = 1,labels.size = 2.3)+
  geom_hline(yintercept = 0,col="hotpink3")+
  geom_vline(xintercept = 0,col="hotpink3")+
  ylim(0,1.8)+
  xlim(0,2.8)+
  theme_light()+
  theme(plot.title=element_text(hjust=.5,size=20),
        axis.text = element_text(size=15)
        )+
  labs(title="Top Right",
       )+
  scale_color_manual(values=cols)+ theme(legend.box.background = element_rect(linetype="solid", colour ="hotpink3", size=1.25),
        legend.title = element_text(face="bold", hjust = .5),
        legend.text = element_text(face="bold"))+
  guides(colour=guide_legend("Country"))

```

```{r,out.height="25%",out.width="50%",fig.show='hold',fig.cap="Figure 1"}

ggbiplot(pcs.out,labels = data$car.name,groups=data$origin,obs.scale = 1,labels.size = 2.3)+
  geom_hline(yintercept = 0,col="hotpink3")+
  geom_vline(xintercept = 0,col="hotpink3")+
  ylim(-2.5,0)+
  xlim(-4.5,0)+
  theme_light()+
  theme(plot.title=element_text(hjust=.5,size=20),
        axis.text = element_text(size=15)
        )+
  labs(title="Bottom Left",
       )+
    scale_color_manual(values=cols)+ 
  theme(legend.box.background = element_rect(linetype="solid", colour ="hotpink3", size=1.25),
        legend.title = element_text(face="bold", hjust = .5),
        legend.text = element_text(face="bold"))+
  guides(colour=guide_legend("Country"))


ggbiplot(pcs.out,labels = data$car.name,groups=data$origin,obs.scale = 1,labels.size = 2.3)+
  geom_hline(yintercept = 0,col="hotpink3")+
  geom_vline(xintercept = 0,col="hotpink3")+
  ylim(-3,0)+
  xlim(0,2.8)+
  theme_light()+
  theme(plot.title=element_text(hjust=.5,size=20),
        axis.text = element_text(size=15)
        )+
  labs(title="Bottom Right",
       )+
    scale_color_manual(values=cols)+
   theme(legend.box.background = element_rect(linetype="solid", colour ="hotpink3", size=1.25),
        legend.title = element_text(face="bold", hjust = .5),
        legend.text = element_text(face="bold"))+
  guides(colour=guide_legend("Country"))

```

# Supervised Learning Analysis 

## Appropriateness of a linear model 
```{r}
data.transformed <- data
data.transformed$mpg <- log(data.transformed$mpg,base=10)
data.transformed <- data.transformed[-c(8)]
```


```{r,out.height="23%",out.width="110%",fig.show='hold',fig.align='center'}
ggpairs(data[-c(2,7,8)],aes(color=data$origin))+
  theme_bw()+
  theme(panel.grid=element_blank(),
        plot.title=element_text(hjust=.5,size=20)) + 
  labs(title="Pairplot of numerical features")+
  scale_color_manual(values=brewer.pal(3,"Set1"))

ggpairs(data.transformed[-c(2,7,8)],aes(color=data$origin))+
  theme_bw()+
  theme(panel.grid=element_blank(),
        plot.title=element_text(hjust=.5,size=20),
        plot.subtitle =element_text(hjust=.5,size=15)) + 
  labs(title=" Pairplot of numerical features",
       subtitle = "Log MPG transformed")+
  scale_color_manual(values=brewer.pal(3,"Set1"))
```

```{r}
lr.data <- lm(mpg~.,data=data[-c(8)])
summary(lr.data)
#residuals vs fitted plot 
p1 <- ggplot(lr.data)+
  theme_light()+
  labs(title = "Residuals Fitted Values",x="Fitted Values",y="Residuals")+
  geom_point(aes(x=lr.data$fitted.values,y=lr.data$residuals),col="darkslateblue",pch=21,fill="turquoise3",alpha=.85,size=2.5,stroke=0.75)+
  geom_hline(yintercept = 0)+
  theme(axis.title = element_text(size=15),
        axis.text = element_text(size=10),
        plot.title = element_text(hjust = .5, size = 15))

p2 <- ggplot(lr.data,aes(sample=lr.data$residuals))+
  labs(title = "QQ Normality Plot",x="Theoretical Quantile",y="Residuals")+
  theme_light()+
  stat_qq(col="darkslateblue",pch=21,fill="turquoise3",alpha=.75,size=2.5,stroke=0.5)+
  geom_qq_line()+
  theme(axis.title = element_text(size=15),
        axis.text = element_text(size=10),
        plot.title = element_text(hjust = .5, size = 20))

```

```{r}
log.lr.data <- lm(mpg~.+I(horsepower^2),data=data.transformed)
summary(log.lr.data)
#residuals vs fitted plot
p1a <- ggplot(log.lr.data)+
  labs(title = "Log Transformed Residuals", x = "Log Fitted Values", y = "Log Residuals")+
  theme_light()+
  geom_point(aes(x=log.lr.data$fitted.values,y=log.lr.data$residuals),col="navyblue",pch=21,fill="violetred1",alpha=.85,size=2.5,stroke=.75)+
  geom_hline(yintercept = 0)+
  theme(axis.title = element_text(size=15),
        axis.text = element_text(size=10),
        plot.title = element_text(hjust = .5, size = 15))

  
p2a <- ggplot(log.lr.data,aes(sample=log.lr.data$residuals))+
  labs(title = "QQ Normality Plot",x="Theoretical Quantile",y="Log Residuals")+
  theme_light()+
  stat_qq(col="navyblue",pch=21,fill="violetred1",alpha=.75,size=2.5,stroke=0.5)+
  geom_qq_line()+
  theme(axis.title = element_text(size=15),
        axis.text = element_text(size=10),
        plot.title = element_text(hjust = .5, size = 20))

```
```{r}
10^.06376
```

```{r}
plot_grid(p1, p2, p1a, p2a)
```

```{r,include=F}
library(MASS)
step.model <- stepAIC(log.lr.data, direction = "both",
trace = FALSE)
summary(step.model)
```

```{r}
colnames(data)
```


## Predictive model 

```{r}

train.test <- function(data,split.size){
  #randomize the data
  randomized.rows <- sample(nrow(data))
  randomized.data <- data[randomized.rows,]
  #split based on desired size
  split <- round(nrow(randomized.data)*split.size)
  train <- randomized.data[1:split,]
  test <- randomized.data[(split+1):nrow(randomized.data),]
  return(list(train,test))
}

#computes the Rsquared and MSE
model.metrics <- function(predicted,actual,data){
  SSE <- sum((predicted-actual)^2)
  SSTO <- sum((actual-mean(actual))^2)
  R.squared <- 1-(SSE/SSTO)
  R.MSE <- sqrt(SSE/nrow(data))
  results <- c(R.MSE,R.squared)
  names(results) <- c("RMSE","R.squared")
  return(results)
}

#From the full model:mpg~.+I(horsepower^2), specify what features to remove
build.model.features <- function(data,feats="None"){
  if(sum(!feats%in%"None")!=0) {
  #input validation
  if(sum(!feats %in% colnames(data))!=0){
    return("Error: No Such feature(s)")
  }
  features <- as.formula(paste("mpg~.+I(horsepower^2)-",paste(feats,collapse= "-")))
  return(features)
  
  }
  else return(as.formula(paste("mpg~.+I(horsepower^2)")))
}

# Combines usage of build.model.features and model.metrics to simulate a train-test split evaluation
build.and.evaluate <- function(data,split.size,feats="None"){
  #train-test split
  train <- train.test(data,split.size)[[1]]
  test <- train.test(data,split.size)[[2]]
  #build model
  model <- lm(build.model.features(data,feats),train)
  print(build.model.features(data,feats))
  #predict on test set
  p.train <- predict(model,train)
  p.test <- predict(model,test)
  #evaluate model
  metric.results <- c(model.metrics(p.train,train$mpg,train),
                      model.metrics(p.test,test$mpg,test))
  names(metric.results) <- c("Train.RMSE","Train.R.Squared","Test.RMSE","Test.R.Squared")
  return(metric.results)
}

# Runs build and evaluate n times and returns a dataframe of the results 
n.build.and.evaluate <- function(n,data,split.size,feats="None"){
  df <- data.frame(matrix(ncol=4,nrow = 0))
  for(i in 1:n){
    metric.results <- build.and.evaluate(data,split.size,feats)
    df <- rbind(df,metric.results)
  }
  df <- cbind(1:n,df)
  colnames(df) <- c("Trial.number","Train.RMSE","Train.R.Squared","Test.RMSE","Test.R.Squared")
  return(df)
}

```



```{r,include=F}
b <- n.build.and.evaluate(100,data.transformed,.8)
avg.b.RMSE <- round(mean(b$Test.RMSE),5)
avg.b.Rsq <- round(mean(b$Test.R.Squared),5)
avg.tr.RMSE <- round(mean(b$Train.RMSE),5)
avg.tr.Rsq <- round(mean(b$Train.R.Squared),5)
```

```{r}
colMeans(b[-c(1)])
```




```{r,out.width="50%",out.height="50%",fig.show='hold'}
ggplot(data=b,aes(x=Trial.number))+
  labs(title="Train-Test n Times: RMSE", x="Trial Number", y="RMSE")+
  theme_light()+
  geom_line(aes(y=Train.RMSE,col="Train.RMSE"))+
  geom_line(aes(y=Test.RMSE,col="Test.RMSE"))+
  coord_cartesian(xlim=c(0,100),ylim=c(0.045,.08))+
  scale_x_continuous(breaks=seq(0,100,5))+
  scale_y_continuous(breaks=seq(0.045,0.08,0.005))+
  scale_color_manual(values = c(Train.RMSE="#E31A1C",Test.RMSE="#33A02C"), labels = c("Train", "Test"))+ 
  theme(legend.box.background = element_rect(linetype="solid", colour ="#984EA3", size=1.25),
        legend.title = element_text(face="bold", hjust = .5),
        legend.text = element_text(face="bold"),
        panel.grid.minor.x = element_blank(),
        axis.title = element_text(size=15),
        axis.text = element_text(size=10),
        plot.title = element_text(hjust = .5, size = 20))+
  guides(colour=guide_legend("RMSE"))+
  geom_hline(yintercept = avg.b.RMSE,col='dodgerblue',linetype='dashed')+
  geom_text(aes(0,avg.b.RMSE,label = avg.b.RMSE, vjust = -.9))+
  geom_hline(yintercept = avg.tr.RMSE,col='black',linetype='dashed')+
  geom_text(aes(0,avg.tr.RMSE,label = avg.tr.RMSE, vjust = 1.5))

ggplot(data=b,aes(x=Trial.number))+
  labs(title="Train-Test n Times: R Squared", x="Trial Number", y="R Squared")+
  theme_light()+
  geom_line(aes(y=Train.R.Squared,col="Train.R.Squared"))+
  geom_line(aes(y=Test.R.Squared,col="Test.R.Squared"))+
  scale_color_manual(values = c(Train.R.Squared="#E31A1C",Test.R.Squared="#33A02C"), labels = c("Train", "Test"))+ 
  coord_cartesian(xlim=c(0,100),ylim=c(.5,1))+
  scale_x_continuous(breaks=seq(0,100,5))+
  scale_y_continuous(breaks=seq(.5,1,0.05))+
  theme(legend.position="right",
    legend.box.background = element_rect(linetype="solid", colour ="#984EA3", size=1.25),
        legend.title = element_text(face="bold", hjust = .5),
        legend.text = element_text(face="bold"),
        panel.grid.minor.x = element_blank(),
        axis.title = element_text(size=15),
        axis.text = element_text(size=10),
        plot.title = element_text(hjust = .5, size = 20))+
  guides(colour=guide_legend("R Squared"))+
  geom_hline(yintercept = avg.b.Rsq,col='dodgerblue',linetype='dashed')+
  geom_text(aes(0,avg.b.Rsq,label = avg.b.Rsq, vjust = 1.5))+
  geom_hline(yintercept = avg.tr.Rsq,col='black',linetype='dashed')+
  geom_text(aes(0,avg.tr.Rsq,label = avg.tr.Rsq, vjust = -.9))

```



## Cross-Fold validation

```{r,include=F}
model <- train(
  build.model.features(data.transformed), 
  data.transformed,
  method = "lm",
  trControl = trainControl(
    method = "repeatedcv", 
    number = 10,
    repeats = 10,
    verboseIter = TRUE
  )
)
```
```{r}
knitr::kable(model$results) %>% kable_styling(font_size = 7)
```

```{r}
kable(model$finalModel$coefficients) %>% kable_styling(font_size = 6)

```








# Conclusion 

# References 

1. Transportation Technologies and Innovation. Union of Concerned Scientists. (n.d.). Retrieved November 12, 2021, from    https://www.ucsusa.org/transportation/technologies. 

2. McGregor, H. V., Gergis, J., Abram, N. J., & Phipps, S. J. (2016). The Industrial Revolution kick-started global warming much earlier than we realised.

3. Learning, U. C. I. M. (2017, July 2). Auto-mpg dataset. Kaggle. Retrieved November 12, 2021, from https://www.kaggle.com/uciml/autompg-dataset. 

# Appendix: R code used

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```